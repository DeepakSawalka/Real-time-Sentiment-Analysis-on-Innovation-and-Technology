# -*- coding: utf-8 -*-
"""Tweet extraction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15fieN0sokLJKQLnmgnUryiZYjXhX2M6G
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install snscrape

import pandas as pd
from datetime import datetime, timedelta
from snscrape.modules import twitter

# Define the date range for the tweets
start_date = datetime(2018, 6, 1).date()
end_date = datetime(2018, 12, 31).date()

# Define your query keywords
query_list = ['#ChatGPT', '#BardAI','#Artificial Intelligence','#AI','#BigData','#MachineLearning','#Robotics']

# Define a function to get tweets for a specific date range
def get_tweets_by_date_range(query, start_date, end_date, limit):
    tweets = []
    for tweet in twitter.TwitterSearchScraper(f"{query} since:{start_date} until:{end_date} lang:en").get_items():
        if len(tweets) >= limit:
            break
        tweets.append(tweet)
    return tweets

# Get the tweets and create a dataframe
tweets_list = []
for query in query_list:
    tweets = get_tweets_by_date_range(query, start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'), 3000)
    for tweet in tweets:
        tweet_dict = {}
        tweet_dict["ID"] = tweet.user.id
        tweet_dict["UserName"] = tweet.user.username
        tweet_dict["Tweets"] = tweet.rawContent
        tweet_dict["Date"] = tweet.date
        tweet_dict["Source"] = tweet.sourceLabel
            
        tweets_list.append(tweet_dict)

df = pd.DataFrame(tweets_list, columns=["ID", "UserName", "Tweets", "Date", "Source"])
df

df.to_csv('/content/drive/MyDrive/58.csv',index=False)

